{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition siamese model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, RandomHorizontalFlip, Resize\n",
    "\n",
    "from torchdata.datapipes.iter import Zipper, IterableWrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEOPLE = ('divy', 'matt', 'arnav', 'mann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    Resize((105, 105)),\n",
    "])\n",
    "\n",
    "# maps: \"person\" -> datasets.ImageFolder\n",
    "full_dataset = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    full_dataset[person] = datasets.ImageFolder(root=os.path.join('data', person), transform=img_transforms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = [img_paths, labels]\n",
    "\n",
    "Labels:\n",
    "- anchor = 0\n",
    "- positive = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps \"person\" -> array of booleans of where the anchor, positive, and negative images are in the dataset\n",
    "is_anchor = {}\n",
    "is_positive = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    is_anchor[person] = torch.tensor(full_dataset[person].targets) == 0\n",
    "    is_positive[person] = torch.tensor(full_dataset[person].targets) == 1\n",
    "\n",
    "# extract the anchor, positive, and negative img indices\n",
    "anchor_indices = {}\n",
    "positive_indices = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    anchor_indices[person] = is_anchor[person].nonzero().flatten()\n",
    "    positive_indices[person] = is_positive[person].nonzero().flatten()\n",
    "\n",
    "# create the anchor, positive, and negative datasets\n",
    "anchor_dataset = {}\n",
    "positive_dataset = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    anchor_dataset[person] = Subset(full_dataset[person], anchor_indices[person])\n",
    "    positive_dataset[person] = Subset(full_dataset[person], positive_indices[person])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the datasets are [(img, label), (img, label), (img, label), ...]. We need them to be [img, img, img, ...] only, no label needed since they are already split by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this takes about 2 minutes to run, please only run it once\n",
    "for person in PEOPLE:\n",
    "    anchor_dataset[person] = [sublist[0] for sublist in list(anchor_dataset[person])]\n",
    "    positive_dataset[person] = [sublist[0] for sublist in list(positive_dataset[person])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip other people's positives to make each negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_dataset = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    negative_dataset[person] = []\n",
    "    for other_person in PEOPLE:\n",
    "        if other_person == person:\n",
    "            continue\n",
    "        negative_dataset[person].extend(positive_dataset[other_person])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing raw datasets (testing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when testing\n",
    "# ds = negative_dataset['divy']\n",
    "# total = 100 # number of images to show\n",
    "\n",
    "# figure, axis = plt.subplots(total // 10, 10, figsize=(16, total // 5))\n",
    "\n",
    "# for i in range(total):\n",
    "#     row = i // 10\n",
    "#     col = i % 10 \n",
    "#     axis[row, col].imshow(ds[i].squeeze(0).permute(1, 2, 0))\n",
    "#     axis[row, col].set_xticks([])\n",
    "#     axis[row, col].set_yticks([])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip anchor, positive, and negative datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_pos_dataset = {}\n",
    "zipped_neg_dataset = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    zipped_pos_dataset[person] = Zipper(IterableWrapper(anchor_dataset[person]), IterableWrapper(positive_dataset[person]), IterableWrapper(torch.ones(len(anchor_dataset[person]))))\n",
    "    zipped_neg_dataset[person] = Zipper(IterableWrapper(anchor_dataset[person]), IterableWrapper(negative_dataset[person]), IterableWrapper(torch.zeros(len(anchor_dataset[person]))))\n",
    "\n",
    "    zipped_pos_dataset[person] = list(zipped_pos_dataset[person])\n",
    "    zipped_neg_dataset[person] = list(zipped_neg_dataset[person])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `zipped_pos_dataset` has the format: `[(anchor, positive, 1), (anchor, positive, 1), ...]`. Here the 1 is the label of the pair, which signifies that the image is positive, meaning from the same person as anchor. So the `zipped_neg_dataset` has 0s instead of 1s and negative images instead of positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the positive and negative datasets and shuffle them\n",
    "final_dataset = []\n",
    "for person in PEOPLE:\n",
    "    final_dataset += zipped_pos_dataset[person] + zipped_neg_dataset[person]\n",
    "\n",
    "np.random.shuffle(final_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulizing final datasets (testing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when testing\n",
    "# ds = final_dataset\n",
    "# total = 100 # number of images to show\n",
    "\n",
    "# figure, axis = plt.subplots(total // 10, 10, figsize=(16, total // 5))\n",
    "\n",
    "# for i in range(total):\n",
    "#     row = i // 10\n",
    "#     col = i % 10 \n",
    "#     axis[row, col].imshow(ds[i][1].squeeze(0).permute(1, 2, 0))\n",
    "#     axis[row, col].set_xticks([])\n",
    "#     axis[row, col].set_yticks([])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# split between training and testing 80-20\n",
    "train_set, test_set = random_split(final_dataset, [int(len(final_dataset) * 0.8), len(final_dataset) - int(len(final_dataset) * 0.8)])\n",
    "train_dataloader : DataLoader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader : DataLoader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(dataloader) = number of batches = total imgs in final_dataset / batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is organized inside `dataloader` as follows: [anchors, pos/neg imgs, label]\n",
    "- img is of shape [3, 224, 224], since there is _batch_size_ (currently 64) images in a batch, the shape of anc/pos/neg imgs is [64, 3, 224, 224]\n",
    "- label is either 0 or 1: 0 for negative, 1 for positive\n",
    "\n",
    "Note: run the next block to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = train_dataloader._get_iterator().__next__()\n",
    "\n",
    "# N = batch size, C = color channels, H = height, W = width\n",
    "print(\"Shape of data [N, C, H, W]: \", first_batch[0].shape)\n",
    "print(\"Shape of labels: \", first_batch[2].shape, first_batch[1].dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the data for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first image is at first_batch[0][0]\n",
    "# batch[i][j] where i is type (0=anchor vs 1=pos/neg) and j is image index in the batch\n",
    "\n",
    "# Uncomment line below when testing\n",
    "# plt.imshow(first_batch[0][1].permute(1, 2, 0))\n",
    "\n",
    "# How to access different batches:\n",
    "#     it = train_dataloader._get_iterator()\n",
    "#     first_batch = it._next_data()\n",
    "#     second_second = it._next_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        \n",
    "        # layers\n",
    "        # first layer: 3 input channels, 64 output channels\n",
    "        self.l1 = nn.Conv2d(3, 64, 10, padding=1)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.p1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # second layer: 64 input channels, 128 output channels\n",
    "        self.l2 = nn.Conv2d(64, 128, 7, padding=1)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.p2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # third layer: 128 input channels, 128 output channels\n",
    "        self.l3 = nn.Conv2d(128, 128, 4, padding=1)\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.p3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # fourth layer: 128 input channels, 256 output channels\n",
    "        self.l4 = nn.Conv2d(128, 256, 4, padding=1)\n",
    "        self.a4 = nn.ReLU()\n",
    "        self.p4 = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass the input tensor through the embeddding network.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor, 3 channels, 105x105 pixels\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor, 4096 channels\n",
    "        \"\"\"\n",
    "        x = self.l1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.p1(x)\n",
    "        \n",
    "        x = self.l2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.p2(x)\n",
    "        \n",
    "        x = self.l3(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.p3(x)\n",
    "        \n",
    "        x = self.l4(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.p4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding_layer = EmbeddingNetwork()\n",
    "        \n",
    "        # fully connected classification layer\n",
    "        # 2 classes: 0 (negative) and 1 (positive)\n",
    "        self.feature_vector = nn.Linear(20736, 4096)\n",
    "        self.classification_layer = nn.Linear(4096, 2)\n",
    "        \n",
    "    def forward(self, anchor, db_image):\n",
    "        \"\"\"Pass the input tensor through the siamese network.\n",
    "\n",
    "        Args:\n",
    "            anchor (torch.Tensor): input image (from webcam), 3 channels, 105x105 pixels\n",
    "            db_image (torch.Tensor): target image (from database), 3 channels, 105x105 pixels\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor, 1 channel\n",
    "        \"\"\"\n",
    "        # pass through embedding layer\n",
    "        anchor = self.embedding_layer(anchor)\n",
    "        db_image = self.embedding_layer(db_image)\n",
    "        \n",
    "        # calculate the absolute difference between the two embeddings\n",
    "        dist = torch.abs(anchor - db_image)\n",
    "        \n",
    "        # pass through fully connected classification layer\n",
    "        x = self.feature_vector(dist)\n",
    "        x = self.classification_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross entropy loss function and adam optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # Get the size of the dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    # Loop over the dataset\n",
    "    for batch, (X, Y, z) in enumerate(dataloader):\n",
    "        X, Y, z = X.to(device), Y.to(device), z.to(device)\n",
    "        \n",
    "        z = z.type(torch.LongTensor)\n",
    "\n",
    "        pred = model(X, Y)\n",
    "        loss = loss_fn(pred, z)\n",
    "\n",
    "        # Backpropagation\n",
    "        # Disable the gradient calculation for the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the gradient of the loss with respect to the model parameters\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss every 3 batches\n",
    "        if batch % 3 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    # Get the size of the dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    # Get the number of batches\n",
    "    num_batches = len(dataloader)\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): # for memory efficiency when testing\n",
    "        # Loop over the dataset\n",
    "        for X, Y, z in dataloader:\n",
    "            X, Y, z = X.to(device), Y.to(device), z.to(device)\n",
    "            z = z.type(torch.LongTensor)\n",
    "            pred = model(X, Y)\n",
    "            test_loss += loss_fn(pred, z).item()\n",
    "            # Add the output value (1 or 0) to the correct variable\n",
    "            correct += (pred.argmax(1) == z).type(torch.float).sum().item()\n",
    "    # Compute the average loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}: -------------------------------\")\n",
    "    # Train the model\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    # Test the model\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in a file, we will use it in the next cell\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "en = enumerate(test_dataloader)\n",
    "x, y, z = en.__next__()[1]\n",
    "\n",
    "print(len(test_dataloader.dataset)/batch_size)\n",
    "\n",
    "for j in range(int(len(test_dataloader.dataset)/batch_size)-1):\n",
    "    x, y, z = en.__next__()[1]\n",
    "    for i in range(batch_size):\n",
    "        with torch.no_grad():\n",
    "            pred = model(x[i].unsqueeze(0), y[i].unsqueeze(0))\n",
    "            predicted, actual = pred[0].argmax(0), z[0]\n",
    "            # print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing tests with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "ds = test_dataloader.dataset\n",
    "\n",
    "fail_counter: int = 0\n",
    "pass_counter: int = 0\n",
    "total = int(len(ds)) - 1 # Number of test_data points to be used\n",
    "model_results = {}\n",
    "\n",
    "for i in range(total):\n",
    "    x, y, z = ds[i][0], ds[i][1], ds[i][2]\n",
    "    with torch.no_grad():\n",
    "        pred = model(x.unsqueeze(0), y.unsqueeze(0))\n",
    "        predicted, actual = pred[0].argmax(0), z\n",
    "        # print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "        if (predicted != actual):\n",
    "            fail_counter += 1\n",
    "            print(\"Fail on\", i)\n",
    "        else:\n",
    "            pass_counter += 1\n",
    "    model_results[i] = predicted\n",
    "\n",
    "print(\"===============RESULTS================\")\n",
    "print(f'Pass: {pass_counter}, Fail: {fail_counter}, Accuracy: {pass_counter/total * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(total, 2, figsize=(2, total * 2))\n",
    "img_idx = 1 # imgae to show: 0 = anchor, 1 = pos/neg\n",
    "\n",
    "for i in range(total):\n",
    "    row = i\n",
    "    col = 0\n",
    "    axis[row, col].imshow(ds[i][0].squeeze(0).permute(1, 2, 0))\n",
    "    axis[row, col + 1].imshow(ds[i][1].squeeze(0).permute(1, 2, 0))\n",
    "    axis[row, col].set_xticks([])\n",
    "    axis[row, col].set_yticks([])\n",
    "    axis[row, col + 1].set_xticks([])\n",
    "    axis[row, col + 1].set_yticks([])\n",
    "    \n",
    "    result = str(i) + \" : \" + (\"Same\" if int(str(model_results[i])[7]) == 1 else \"Different\")\n",
    "    \n",
    "    axis[row, col].annotate(result, xy=(0.5, 0.5), xytext=(0, -10))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
