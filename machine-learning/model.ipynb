{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition siamese model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, RandomHorizontalFlip, Resize\n",
    "\n",
    "from torchdata.datapipes.iter import Zipper, IterableWrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.join('data')\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    Resize((105, 105)),\n",
    "])\n",
    "full_dataset: datasets.ImageFolder = datasets.ImageFolder(root=ROOT_PATH, transform=img_transforms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = [img_paths, labels]\n",
    "\n",
    "Labels:\n",
    "- anchor = 0\n",
    "- positive = 1\n",
    "- negative = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of booleans of where the anchor, positive, and negative images are in the dataset\n",
    "is_anchor : bool = torch.tensor(full_dataset.targets) == 0\n",
    "is_negative :bool = torch.tensor(full_dataset.targets) == 1\n",
    "is_positive : bool = torch.tensor(full_dataset.targets) == 2\n",
    "\n",
    "# extract the anchor, positive, and negative img indices\n",
    "anchor_indices : torch.Tensor = is_anchor.nonzero().flatten()\n",
    "negative_indices : torch.Tensor = is_negative.nonzero().flatten()\n",
    "positive_indices : torch.Tensor = is_positive.nonzero().flatten()\n",
    "\n",
    "# create the anchor, positive, and negative datasets\n",
    "anchor_dataset : Subset = Subset(full_dataset, anchor_indices)\n",
    "negative_dataset : Subset = Subset(full_dataset, negative_indices)\n",
    "positive_dataset : Subset = Subset(full_dataset, positive_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the datasets are [(img, label), (img, label), (img, label), ...]. We need them to be [img, img, img, ...] only, no label needed since they are already split by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this takes about 2 minutes to run, please only run it once\n",
    "anchor_dataset : list = [sublist[0] for sublist in list(anchor_dataset)]\n",
    "negative_dataset : list = [sublist[0] for sublist in list(negative_dataset)]\n",
    "positive_dataset : list = [sublist[0] for sublist in list(positive_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip the anchor, positive, and negative datasets together\n",
    "zipped_pos_dataset : list = Zipper(IterableWrapper(anchor_dataset), IterableWrapper(positive_dataset), IterableWrapper(torch.ones(len(anchor_dataset))))\n",
    "zipped_neg_dataset : list = Zipper(IterableWrapper(anchor_dataset), IterableWrapper(negative_dataset), IterableWrapper(torch.zeros(len(anchor_dataset))))\n",
    "\n",
    "zipped_pos_dataset : list = list(zipped_pos_dataset)\n",
    "zipped_neg_dataset : list = list(zipped_neg_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `zipped_pos_dataset` has the format: `[(anchor, positive, 1), (anchor, positive, 1), ...]`. Here the 1 is the label of the pair, which signifies that the image is positive, meaning from the same person as anchor. So the `zipped_neg_dataset` has 0s instead of 1s and negative images instead of positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the positive and negative datasets and shuffle them\n",
    "final_dataset : list = zipped_pos_dataset + zipped_neg_dataset\n",
    "np.random.shuffle(final_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# split between training and testing 80-20\n",
    "train_set, test_set = random_split(final_dataset, [int(len(final_dataset) * 0.8), len(final_dataset) - int(len(final_dataset) * 0.8)])\n",
    "train_dataloader : DataLoader = DataLoader(train_set, batch_size=batch_size)\n",
    "test_dataloader : DataLoader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(dataloader) = number of batches = total imgs in final_dataset / batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is organized inside `dataloader` as follows: [anchors, pos/neg imgs, label]\n",
    "- img is of shape [3, 224, 224], since there is _batch_size_ (currently 64) images in a batch, the shape of anc/pos/neg imgs is [64, 3, 224, 224]\n",
    "- label is either 0 or 1: 0 for negative, 1 for positive\n",
    "\n",
    "Note: run the next block to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = train_dataloader._get_iterator().__next__()\n",
    "\n",
    "# N = batch size, C = color channels, H = height, W = width\n",
    "print(\"Shape of data [N, C, H, W]: \", first_batch[0].shape)\n",
    "print(\"Shape of labels: \", first_batch[2].shape, first_batch[1].dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data for debugging\n",
    "\n",
    "Here is an example of how to visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first image is at first_batch[0][0]\n",
    "# we need to change tesor ordering for plt.imshow using permute\n",
    "plt.imshow(first_batch[0][0].permute(1, 2, 0))\n",
    "\n",
    "# How to access different batches:\n",
    "# it = train_dataloader._get_iterator()\n",
    "# first_batch = it._next_data()\n",
    "# second_second = it._next_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        \n",
    "        # layers\n",
    "        # first layer: 3 input channels, 64 output channels\n",
    "        self.l1 = nn.Conv2d(3, 64, 10, padding=1)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.p1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # second layer: 64 input channels, 128 output channels\n",
    "        self.l2 = nn.Conv2d(64, 128, 7, padding=1)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.p2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # third layer: 128 input channels, 128 output channels\n",
    "        self.l3 = nn.Conv2d(128, 128, 4, padding=1)\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.p3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # fourth layer: 128 input channels, 256 output channels\n",
    "        self.l4 = nn.Conv2d(128, 256, 4, padding=1)\n",
    "        self.a4 = nn.ReLU()\n",
    "        self.p4 = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass the input tensor through the embeddding network.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor, 3 channels, 105x105 pixels\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor, 4096 channels\n",
    "        \"\"\"\n",
    "        x = self.l1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.p1(x)\n",
    "        \n",
    "        x = self.l2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.p2(x)\n",
    "        \n",
    "        x = self.l3(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.p3(x)\n",
    "        \n",
    "        x = self.l4(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.p4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding_layer = EmbeddingNetwork()\n",
    "        \n",
    "        # fully connected classification layer\n",
    "        # 2 classes: 0 (negative) and 1 (positive)\n",
    "        self.feature_vector = nn.Linear(20736, 4096)\n",
    "        self.classification_layer = nn.Linear(4096, 2)\n",
    "        \n",
    "    def forward(self, anchor, db_image):\n",
    "        \"\"\"Pass the input tensor through the siamese network.\n",
    "\n",
    "        Args:\n",
    "            anchor (torch.Tensor): input image (from webcam), 3 channels, 105x105 pixels\n",
    "            db_image (torch.Tensor): target image (from database), 3 channels, 105x105 pixels\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor, 1 channel\n",
    "        \"\"\"\n",
    "        # pass through embedding layer\n",
    "        anchor = self.embedding_layer(anchor)\n",
    "        db_image = self.embedding_layer(db_image)\n",
    "        \n",
    "        # calculate the absolute difference between the two embeddings\n",
    "        dist = torch.abs(anchor - db_image)\n",
    "        \n",
    "        # pass through fully connected classification layer\n",
    "        x = self.feature_vector(dist)\n",
    "        x = self.classification_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseNetwork(\n",
      "  (embedding_layer): EmbeddingNetwork(\n",
      "    (l1): Conv2d(3, 64, kernel_size=(10, 10), stride=(1, 1), padding=(1, 1))\n",
      "    (a1): ReLU()\n",
      "    (p1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (l2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\n",
      "    (a2): ReLU()\n",
      "    (p2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (l3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (a3): ReLU()\n",
      "    (p3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (l4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (a4): ReLU()\n",
      "    (p4): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (feature_vector): Linear(in_features=20736, out_features=4096, bias=True)\n",
      "  (classification_layer): Linear(in_features=4096, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross entropy loss function and adam optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # Get the size of the dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    # Loop over the dataset\n",
    "    for batch, (X, Y, z) in enumerate(dataloader):\n",
    "        X, Y, z = X.to(device), Y.to(device), z.to(device)\n",
    "        \n",
    "        z = z.type(torch.LongTensor)\n",
    "\n",
    "        pred = model(X, Y)\n",
    "        loss = loss_fn(pred, z)\n",
    "\n",
    "        # Backpropagation\n",
    "        # Disable the gradient calculation for the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the gradient of the loss with respect to the model parameters\n",
    "        loss.backward()\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss every 100 batches\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    # Get the size of the dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    # Get the number of batches\n",
    "    num_batches = len(dataloader)\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        # Loop over the dataset\n",
    "        for X, Y, z in dataloader:\n",
    "            X, Y, z = X.to(device), Y.to(device), z.to(device)\n",
    "            z = z.type(torch.LongTensor)\n",
    "            pred = model(X, Y)\n",
    "            test_loss += loss_fn(pred, z).item()\n",
    "            # Add the output value (1 or 0) to the correct variable\n",
    "            correct += (pred.argmax(1) == z).type(torch.float).sum().item()\n",
    "    # Compute the average loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.703034  [    0/  223]\n",
      "loss: 1.996190  [  100/  223]\n",
      "loss: 0.240953  [  200/  223]\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.298442 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.347941  [    0/  223]\n",
      "loss: 1.526834  [  100/  223]\n",
      "loss: 0.267948  [  200/  223]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.333829 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.598308  [    0/  223]\n",
      "loss: 0.019489  [  100/  223]\n",
      "loss: 0.052453  [  200/  223]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.238684 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.210626  [    0/  223]\n",
      "loss: 0.008171  [  100/  223]\n",
      "loss: 0.032453  [  200/  223]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.343465 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.014554  [    0/  223]\n",
      "loss: 0.382451  [  100/  223]\n",
      "loss: 0.423024  [  200/  223]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.232769 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    # Train the model\n",
    "    train(train_dataloader, model, loss_fn, optimizer)p\n",
    "    # Test the model\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "# Saving the model in a file, we will use it in the next cell\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 105, 105]) torch.Size([4, 3, 105, 105]) torch.Size([4])\n",
      "Predicted: \"0\", Actual: \"0.0\"\n",
      "Predicted: \"1\", Actual: \"0.0\"\n",
      "Predicted: \"0\", Actual: \"0.0\"\n",
      "Predicted: \"0\", Actual: \"0.0\"\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "x, y, z = enumerate(test_dataloader).__next__()[1]\n",
    "\n",
    "print(x.shape, y.shape, z.shape)\n",
    "\n",
    "# plt.imshow(x.permute(1, 2, 0))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    with torch.no_grad():\n",
    "        pred = model(x[i].unsqueeze(0), y[i].unsqueeze(0))\n",
    "        predicted, actual = pred[0].argmax(0), z[0]\n",
    "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
