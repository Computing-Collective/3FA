{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition siamese model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, RandomHorizontalFlip, Resize\n",
    "\n",
    "from torchdata.datapipes.iter import Zipper, IterableWrapper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.join('data')\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    Resize((105, 105)),\n",
    "])\n",
    "full_dataset: datasets.ImageFolder = datasets.ImageFolder(root=ROOT_PATH, transform=img_transforms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = [img_paths, labels]\n",
    "\n",
    "Labels:\n",
    "- anchor = 0\n",
    "- positive = 1\n",
    "- negative = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of booleans of where the anchor, positive, and negative images are in the dataset\n",
    "is_anchor : bool = torch.tensor(full_dataset.targets) == 0\n",
    "is_negative :bool = torch.tensor(full_dataset.targets) == 1\n",
    "is_positive : bool = torch.tensor(full_dataset.targets) == 2\n",
    "\n",
    "# extract the anchor, positive, and negative img indices\n",
    "anchor_indices : torch.Tensor = is_anchor.nonzero().flatten()\n",
    "negative_indices : torch.Tensor = is_negative.nonzero().flatten()\n",
    "positive_indices : torch.Tensor = is_positive.nonzero().flatten()\n",
    "\n",
    "# create the anchor, positive, and negative datasets\n",
    "anchor_dataset : Subset = Subset(full_dataset, anchor_indices)\n",
    "negative_dataset : Subset = Subset(full_dataset, negative_indices)\n",
    "positive_dataset : Subset = Subset(full_dataset, positive_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the datasets are [(img, label), (img, label), (img, label), ...]. We need them to be [img, img, img, ...] only, no label needed since they are already split by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this takes about 1.5 minutes to run, please only run it once\n",
    "anchor_dataset : list = [sublist[0] for sublist in list(anchor_dataset)]\n",
    "negative_dataset : list = [sublist[0] for sublist in list(negative_dataset)]\n",
    "positive_dataset : list = [sublist[0] for sublist in list(positive_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip the anchor, positive, and negative datasets together\n",
    "zipped_pos_dataset : list = Zipper(IterableWrapper(anchor_dataset), IterableWrapper(positive_dataset), IterableWrapper(torch.ones(len(anchor_dataset))))\n",
    "zipped_neg_dataset : list = Zipper(IterableWrapper(anchor_dataset), IterableWrapper(negative_dataset), IterableWrapper(torch.zeros(len(anchor_dataset))))\n",
    "\n",
    "zipped_pos_dataset : list = list(zipped_pos_dataset)\n",
    "zipped_neg_dataset : list = list(zipped_neg_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `zipped_pos_dataset` has the format: `[(anchor, positive, 1), (anchor, positive, 1), ...]`. Here the 1 is the label of the pair, which signifies that the image is positive, meaning from the same person as anchor. So the `zipped_neg_dataset` has 0s instead of 1s and negative images instead of positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the positive and negative datasets and shuffle them\n",
    "final_dataset : list = zipped_pos_dataset + zipped_neg_dataset\n",
    "np.random.shuffle(final_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# split between training and testing 80-20\n",
    "train_set, test_set = random_split(final_dataset, [int(len(final_dataset) * 0.8), len(final_dataset) - int(len(final_dataset) * 0.8)])\n",
    "train_dataloader : DataLoader = DataLoader(train_set, batch_size=batch_size)\n",
    "test_dataset : DataLoader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(dataloader) = number of batches = total imgs in final_dataset / batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is organized inside `dataloader` as follows: [anchors, pos/neg imgs, label]\n",
    "- img is of shape [3, 224, 224], since there is _batch_size_ (currently 64) images in a batch, the shape of anc/pos/neg imgs is [64, 3, 224, 224]\n",
    "- label is either 0 or 1: 0 for negative, 1 for positive\n",
    "\n",
    "Note: run the next block to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = train_dataloader._get_iterator().__next__()\n",
    "\n",
    "# N = batch size, C = color channels, H = height, W = width\n",
    "print(\"Shape of data [N, C, H, W]: \", first_batch[0].shape)\n",
    "print(\"Shape of labels: \", first_batch[2].shape, first_batch[1].dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data for debugging\n",
    "\n",
    "Here is an example of how to visualize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first image is at first_batch[0][0]\n",
    "# we need to change tesor ordering for plt.imshow using permute\n",
    "plt.imshow(first_batch[0][0].permute(1, 2, 0))\n",
    "\n",
    "# How to access different batches:\n",
    "# it = train_dataloader._get_iterator()\n",
    "# first_batch = it._next_data()\n",
    "# second_second = it._next_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        \n",
    "        # layers\n",
    "        # first layer: 3 input channels, 64 output channels\n",
    "        self.l1 = nn.Conv2d(3, 64, 10, padding=1)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.p1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # second layer: 64 input channels, 128 output channels\n",
    "        self.l2 = nn.Conv2d(64, 128, 7, padding=1)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.p2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # third layer: 128 input channels, 128 output channels\n",
    "        self.l3 = nn.Conv2d(128, 128, 4, padding=1)\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.p3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # fourth layer: 128 input channels, 256 output channels\n",
    "        self.l4 = nn.Conv2d(128, 256, 4, padding=1)\n",
    "        self.a4 = nn.ReLU()\n",
    "        self.p4 = nn.Flatten()\n",
    "        \n",
    "        # dense layer: 256 input channels, 4096 output channels\n",
    "        self.l5 = nn.Linear(256, 4096)\n",
    "        self.a5 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass the input tensor through the embeddding network.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor, 3 channels, 105x105 pixels\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor, 4096 channels\n",
    "        \"\"\"\n",
    "        # pass values through layers\n",
    "        x = self.l1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.p3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.p4(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding_layer = EmbeddingNetwork()\n",
    "        \n",
    "        # fully connected classification layer\n",
    "        self.classification_layer = nn.Linear(4096, 1)\n",
    "        \n",
    "    def forward(self, x_input, x_target):\n",
    "        \"\"\"Pass the input tensor through the siamese network.\n",
    "\n",
    "        Args:\n",
    "            x_input (torch.Tensor): input image (from webcam), 3 channels, 105x105 pixels\n",
    "            x_target (torch.Tensor): target image (from database), 3 channels, 105x105 pixels\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor, 1 channel\n",
    "        \"\"\"\n",
    "        # pass through embedding layer\n",
    "        x_input = self.embedding_layer(x_input)\n",
    "        x_target = self.embedding_layer(x_target)\n",
    "        \n",
    "        # calculate the absolute difference between the two embeddings\n",
    "        dist = torch.abs(x_input - x_target)\n",
    "        \n",
    "        # pass through fully connected classification layer\n",
    "        x = self.classification_layer(dist)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork().to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross entropy loss function and adam optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
