{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition siamese model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset\n",
    "from torchdata.datapipes.iter import Zipper, IterableWrapper\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose, RandomHorizontalFlip, Resize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"data\")\n",
    "PEOPLE = tuple([name for name in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, name))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    Resize((105, 105), antialias=None),\n",
    "])\n",
    "\n",
    "# maps: \"person\" -> datasets.ImageFolder\n",
    "full_dataset = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    full_dataset[person] = datasets.ImageFolder(root=os.path.join('data', person), transform=img_transforms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = [img_paths, labels]\n",
    "\n",
    "Labels:\n",
    "- anchor = 0\n",
    "- positive = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps \"person\" -> array of booleans of where the anchor, positive, and negative images are in the dataset\n",
    "is_anchor = {}\n",
    "is_positive = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    is_anchor[person] = torch.tensor(full_dataset[person].targets) == 0\n",
    "    is_positive[person] = torch.tensor(full_dataset[person].targets) == 1\n",
    "\n",
    "# extract the anchor, positive, and negative img indices\n",
    "anchor_indices = {}\n",
    "positive_indices = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    anchor_indices[person] = is_anchor[person].nonzero().flatten()\n",
    "    positive_indices[person] = is_positive[person].nonzero().flatten()\n",
    "\n",
    "# create the anchor, positive, and negative datasets\n",
    "anchor_dataset = {}\n",
    "positive_dataset = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    anchor_dataset[person] = Subset(full_dataset[person], anchor_indices[person])\n",
    "    positive_dataset[person] = Subset(full_dataset[person], positive_indices[person])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the datasets are [(img, label), (img, label), (img, label), ...]. We need them to be [img, img, img, ...] only, no label needed since they are already split by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in PEOPLE:\n",
    "    anchor_dataset[person] = [sublist[0] for sublist in list(anchor_dataset[person])]\n",
    "    positive_dataset[person] = [sublist[0] for sublist in list(positive_dataset[person])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip other people's positives to make each negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_dataset = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    negative_dataset[person] = []\n",
    "    for other_person in PEOPLE:\n",
    "        if other_person == person:\n",
    "            continue\n",
    "        negative_dataset[person].extend(positive_dataset[other_person])\n",
    "    np.random.shuffle(negative_dataset[person])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip anchor, positive, and negative datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_pos_dataset = {}\n",
    "zipped_neg_dataset = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    zipped_pos_dataset[person] = Zipper(IterableWrapper(anchor_dataset[person]), IterableWrapper(positive_dataset[person]), IterableWrapper(torch.ones(len(anchor_dataset[person]))))\n",
    "    zipped_neg_dataset[person] = Zipper(IterableWrapper(anchor_dataset[person]), IterableWrapper(negative_dataset[person]), IterableWrapper(torch.zeros(len(anchor_dataset[person]))))\n",
    "\n",
    "    zipped_pos_dataset[person] = list(zipped_pos_dataset[person])\n",
    "    zipped_neg_dataset[person] = list(zipped_neg_dataset[person])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `zipped_pos_dataset` has the format: `[(anchor, positive, 1), (anchor, positive, 1), ...]`. Here the 1 is the label of the pair, which signifies that the image is positive, meaning from the same person as anchor. So the `zipped_neg_dataset` has 0s instead of 1s and negative images instead of positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the positive and negative datasets and shuffle them\n",
    "final_dataset = []\n",
    "for person in PEOPLE:\n",
    "    final_dataset += zipped_pos_dataset[person] + zipped_neg_dataset[person]\n",
    "\n",
    "np.random.shuffle(final_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# split between training and testing 80-20\n",
    "train_set, test_set = random_split(final_dataset, [int(len(final_dataset) * 0.8), len(final_dataset) - int(len(final_dataset) * 0.8)])\n",
    "train_dataloader : DataLoader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader : DataLoader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(dataloader) = number of batches = total imgs in final_dataset / batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is organized inside `dataloader` as follows: [anchors, pos/neg imgs, label]\n",
    "- img is of shape [3, 224, 224], since there is _batch_size_ (currently 64) images in a batch, the shape of anc/pos/neg imgs is [64, 3, 224, 224]\n",
    "- label is either 0 or 1: 0 for negative, 1 for positive\n",
    "\n",
    "Note: run the next block to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_batch = train_dataloader._get_iterator().__next__()\n",
    "\n",
    "# # N = batch size, C = color channels, H = height, W = width\n",
    "# print(\"Shape of data [N, C, H, W]: \", first_batch[0].shape)\n",
    "# print(\"Shape of labels: \", first_batch[2].shape, first_batch[1].dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        \n",
    "        # layers\n",
    "        # first layer: 3 input channels, 64 output channels\n",
    "        self.l1 = nn.Conv2d(3, 64, 10, padding=1)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.p1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # second layer: 64 input channels, 128 output channels\n",
    "        self.l2 = nn.Conv2d(64, 128, 7, padding=1)\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.p2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # third layer: 128 input channels, 128 output channels\n",
    "        self.l3 = nn.Conv2d(128, 128, 4, padding=1)\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.p3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # fourth layer: 128 input channels, 256 output channels\n",
    "        self.l4 = nn.Conv2d(128, 256, 4, padding=1)\n",
    "        self.a4 = nn.ReLU()\n",
    "        self.p4 = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass the input tensor through the embeddding network.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor, 3 channels, 105x105 pixels\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor, 4096 channels\n",
    "        \"\"\"\n",
    "        x = self.l1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.p1(x)\n",
    "        \n",
    "        x = self.l2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.p2(x)\n",
    "        \n",
    "        x = self.l3(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.p3(x)\n",
    "        \n",
    "        x = self.l4(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.p4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding_layer = EmbeddingNetwork()\n",
    "        \n",
    "        # fully connected classification layer\n",
    "        # 2 classes: 0 (negative) and 1 (positive)\n",
    "        self.feature_vector = nn.Linear(20736, 4096)\n",
    "        self.classification_layer = nn.Linear(4096, 2)\n",
    "        \n",
    "    def forward(self, anchor, db_image):\n",
    "        \"\"\"Pass the input tensor through the siamese network.\n",
    "\n",
    "        Args:\n",
    "            anchor (torch.Tensor): input image (from webcam), 3 channels, 105x105 pixels\n",
    "            db_image (torch.Tensor): target image (from database), 3 channels, 105x105 pixels\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor, 1 channel\n",
    "        \"\"\"\n",
    "        # pass through embedding layer\n",
    "        anchor = self.embedding_layer(anchor)\n",
    "        db_image = self.embedding_layer(db_image)\n",
    "        \n",
    "        # calculate the absolute difference between the two embeddings\n",
    "        dist = torch.abs(anchor - db_image)\n",
    "        \n",
    "        # pass through fully connected classification layer\n",
    "        x = self.feature_vector(dist)\n",
    "        x = self.classification_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate a fresh model\n",
    "# model = SiameseNetwork().to(device)\n",
    "\n",
    "# To improve the pre-trained model\n",
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross entropy loss function and adam optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    # Loop over the dataset\n",
    "    for batch, (X, Y, z) in enumerate(dataloader):\n",
    "        X, Y, z = X.to(device), Y.to(device), z.to(device)\n",
    "        z = z.type(torch.LongTensor)\n",
    "\n",
    "        pred = model(X, Y)\n",
    "        loss = loss_fn(pred, z)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss every 3 batches\n",
    "        if batch % 3 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad(): # for memory efficiency when testing\n",
    "        for X, Y, z in dataloader:\n",
    "            X, Y, z = X.to(device), Y.to(device), z.to(device)\n",
    "            z = z.type(torch.LongTensor)\n",
    "\n",
    "            pred = model(X, Y)\n",
    "            test_loss += loss_fn(pred, z).item()\n",
    "            # Add the output value (1 or 0) to the correct variable\n",
    "            correct += (pred.argmax(1) == z).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}: -------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in a file, we will use it for testing\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SiameseNetwork()\n",
    "# model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# en = enumerate(test_dataloader)\n",
    "# x, y, z = en.__next__()[1]\n",
    "\n",
    "# for j in range(int(len(test_dataloader.dataset)/batch_size)-1):\n",
    "#     x, y, z = en.__next__()[1]\n",
    "#     for i in range(batch_size):\n",
    "#         with torch.no_grad():\n",
    "#             pred = model(x[i].unsqueeze(0), y[i].unsqueeze(0))\n",
    "#             predicted, actual = pred[0].argmax(0), z[0]\n",
    "#             print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing tests with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "ds = test_dataloader.dataset\n",
    "\n",
    "fail_counter: int = 0\n",
    "pass_counter: int = 0\n",
    "total = int(len(ds)) - 1 # Number of test_data points to be used\n",
    "model_results = {}\n",
    "incorrect_on = []\n",
    "\n",
    "for i in range(total):\n",
    "    x, y, z = ds[i][0], ds[i][1], ds[i][2]\n",
    "    with torch.no_grad():\n",
    "        pred = model(x.unsqueeze(0), y.unsqueeze(0))\n",
    "        predicted, actual = pred[0].argmax(0), z\n",
    "        # print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "        if (predicted != actual):\n",
    "            fail_counter += 1\n",
    "            incorrect_on.append(i)\n",
    "            print(\"Fail on\", i)\n",
    "        else:\n",
    "            pass_counter += 1\n",
    "    model_results[i] = predicted\n",
    "\n",
    "print(\"===============RESULTS================\")\n",
    "print(f'Pass: {pass_counter}, Fail: {fail_counter}, Accuracy: {pass_counter/total * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_per_row = 4\n",
    "\n",
    "figure, axis = plt.subplots(total // items_per_row - 1, items_per_row * 2, figsize=(items_per_row * 2, 1 * (total // 2)))\n",
    "\n",
    "for i in range(total // items_per_row - 1):\n",
    "    row = i\n",
    "    for j in range(items_per_row):\n",
    "        col = j * 2\n",
    "        idx = i * items_per_row + j\n",
    "        axis[row, col].imshow(ds[idx][0].squeeze(0).permute(1, 2, 0))\n",
    "        axis[row, col + 1].imshow(ds[idx][1].squeeze(0).permute(1, 2, 0))\n",
    "        axis[row, col].set_xticks([])\n",
    "        axis[row, col].set_yticks([])\n",
    "        axis[row, col + 1].set_xticks([])\n",
    "        axis[row, col + 1].set_yticks([])\n",
    "\n",
    "        result = str(idx) + \" : \" + (\"Same\" if int(str(model_results[idx])[7]) == 1 else \"Different\")\n",
    "        result += \" - Wrong\" if i * items_per_row + j in incorrect_on else \"\"\n",
    "        axis[row, col].annotate(result, xy=(0.5, 0.5), xytext=(0, -10))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
